<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="源码是用 python2 &amp; tensorflow1 完成的，一共四个文件。 |    data_iterator.py  数据生成 |    model.py          模型定义 |    train.py        训练和测试 |    utils.py        子网络定义">
<meta property="og:type" content="article">
<meta property="og:title" content="DMR源码解析">
<meta property="og:url" content="http://yoursite.com/2020/05/07/DMR-code/index.html">
<meta property="og:site_name" content="个人博客">
<meta property="og:description" content="源码是用 python2 &amp; tensorflow1 完成的，一共四个文件。 |    data_iterator.py  数据生成 |    model.py          模型定义 |    train.py        训练和测试 |    utils.py        子网络定义">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-05-07T15:03:28.000Z">
<meta property="article:modified_time" content="2020-05-29T07:58:17.779Z">
<meta property="article:author" content="谭廷一">
<meta property="article:tag" content="DMR">
<meta property="article:tag" content="CTR">
<meta property="article:tag" content="code">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/05/07/DMR-code/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>DMR源码解析 | 个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/05/07/DMR-code/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="谭廷一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DMR源码解析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-07 23:03:28" itemprop="dateCreated datePublished" datetime="2020-05-07T23:03:28+08:00">2020-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-29 15:58:17" itemprop="dateModified" datetime="2020-05-29T15:58:17+08:00">2020-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>源码是用 python2 &amp; tensorflow1 完成的，一共四个文件。</p>
<p>|    data_iterator.py  数据生成</p>
<p>|    model.py          模型定义</p>
<p>|    train.py        训练和测试</p>
<p>|    utils.py        子网络定义<br><a id="more"></a></p>
<h1 id="data-iterator-py"><a href="#data-iterator-py" class="headerlink" title="data_iterator.py"></a>data_iterator.py</h1><p>定义了一个 DataIterator 的类用于读取数据，是一个可迭代对象，每次返回一个特征和一个标签。</p>
<p>源码是使用 python 2 写的，将 next 方法修改为 __next__  , xrange 修改为 range，就可以在 python 3 的环境上运行了。</p>
<h1 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># user feature size</span></span><br><span class="line">user_size = <span class="number">1141730</span> <span class="comment"># 用户 ID</span></span><br><span class="line">cms_segid_size = <span class="number">97</span> <span class="comment"># 微群 ID </span></span><br><span class="line">cms_group_id_size = <span class="number">13</span> <span class="comment"># cms_group_id </span></span><br><span class="line">final_gender_code_size = <span class="number">3</span> <span class="comment"># 性别</span></span><br><span class="line">age_level_size = <span class="number">7</span> <span class="comment"># 年龄层次</span></span><br><span class="line">pvalue_level_size = <span class="number">4</span> <span class="comment"># 消费档次</span></span><br><span class="line">shopping_level_size = <span class="number">4</span> <span class="comment"># 购物深度</span></span><br><span class="line">occupation_size = <span class="number">3</span> <span class="comment"># 是否大学生</span></span><br><span class="line">new_user_class_level_size = <span class="number">5</span> <span class="comment"># 城市层级</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># item feature size</span></span><br><span class="line">adgroup_id_size = <span class="number">846812</span> <span class="comment"># 广告单元ID</span></span><br><span class="line">cate_size = <span class="number">12978</span> <span class="comment"># 商品类目</span></span><br><span class="line">campaign_id_size = <span class="number">423437</span> <span class="comment"># 广告计划ID</span></span><br><span class="line">customer_size = <span class="number">255876</span> <span class="comment"># 广告主ID</span></span><br><span class="line">brand_size = <span class="number">461529</span> <span class="comment"># 品牌ID</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># context feature size</span></span><br><span class="line">btag_size = <span class="number">5</span> <span class="comment"># 行为类型</span></span><br><span class="line">pid_size = <span class="number">2</span> <span class="comment"># 资源位</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># embedding size</span></span><br><span class="line">main_embedding_size = <span class="number">32</span> <span class="comment"># 大尺寸</span></span><br><span class="line">other_embedding_size = <span class="number">8</span> <span class="comment"># 小尺寸</span></span><br></pre></td></tr></table></figure>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr, global_step)</span>:</span></span><br><span class="line">        <span class="comment"># input</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Inputs'</span>):</span><br><span class="line">            self.feature_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="literal">None</span>], name=<span class="string">'feature_ph'</span>) <span class="comment"># 特征</span></span><br><span class="line">            self.target_ph = tf.placeholder(tf.float32, [<span class="literal">None</span>, ], name=<span class="string">'target_ph'</span>) <span class="comment"># 标签</span></span><br><span class="line"></span><br><span class="line">            self.btag_his = tf.cast(self.feature_ph[:, <span class="number">0</span>:<span class="number">50</span>], tf.int32) <span class="comment"># 历史行为</span></span><br><span class="line">            self.cate_his = tf.cast(self.feature_ph[:, <span class="number">50</span>:<span class="number">100</span>], tf.int32) <span class="comment"># 历史行为对应的商品类目</span></span><br><span class="line">            self.brand_his = tf.cast(self.feature_ph[:, <span class="number">100</span>:<span class="number">150</span>], tf.int32) <span class="comment"># 历史行为对应的品牌ID</span></span><br><span class="line">            self.mask = tf.cast(self.feature_ph[:, <span class="number">150</span>:<span class="number">200</span>], tf.int32) </span><br><span class="line">            self.match_mask = tf.cast(self.feature_ph[:, <span class="number">200</span>:<span class="number">250</span>], tf.int32) </span><br><span class="line"></span><br><span class="line">            self.uid = tf.cast(self.feature_ph[:, <span class="number">250</span>], tf.int32) <span class="comment"># 用户id</span></span><br><span class="line">            self.cms_segid = tf.cast(self.feature_ph[:, <span class="number">251</span>], tf.int32) <span class="comment"># 微群 ID</span></span><br><span class="line">            self.cms_group_id = tf.cast(self.feature_ph[:, <span class="number">252</span>], tf.int32) <span class="comment"># cms_group_id</span></span><br><span class="line">            self.final_gender_code = tf.cast(self.feature_ph[:, <span class="number">253</span>], tf.int32) <span class="comment"># 性别</span></span><br><span class="line">            self.age_level = tf.cast(self.feature_ph[:, <span class="number">254</span>], tf.int32) <span class="comment"># 年龄层次</span></span><br><span class="line">            self.pvalue_level = tf.cast(self.feature_ph[:, <span class="number">255</span>], tf.int32) <span class="comment"># 消费档次</span></span><br><span class="line">            self.shopping_level = tf.cast(self.feature_ph[:, <span class="number">256</span>], tf.int32) <span class="comment"># 购物深度</span></span><br><span class="line">            self.occupation = tf.cast(self.feature_ph[:, <span class="number">257</span>], tf.int32) <span class="comment"># 是否大学生</span></span><br><span class="line">            self.new_user_class_level = tf.cast(self.feature_ph[:, <span class="number">258</span>], tf.int32) <span class="comment"># 城市层级</span></span><br><span class="line"></span><br><span class="line">            self.mid = tf.cast(self.feature_ph[:, <span class="number">259</span>], tf.int32) <span class="comment"># 广告单元id</span></span><br><span class="line">            self.cate_id = tf.cast(self.feature_ph[:, <span class="number">260</span>], tf.int32) <span class="comment"># 目标商品id</span></span><br><span class="line">            self.campaign_id = tf.cast(self.feature_ph[:, <span class="number">261</span>], tf.int32) <span class="comment"># 广告计划id</span></span><br><span class="line">            self.customer = tf.cast(self.feature_ph[:, <span class="number">262</span>], tf.int32) <span class="comment"># 广告主id</span></span><br><span class="line">            self.brand = tf.cast(self.feature_ph[:, <span class="number">263</span>], tf.int32) <span class="comment"># 品牌id</span></span><br><span class="line">            self.price = tf.expand_dims(tf.cast(self.feature_ph[:, <span class="number">264</span>], tf.float32), <span class="number">1</span>) <span class="comment"># 价格</span></span><br><span class="line"></span><br><span class="line">            self.pid = tf.cast(self.feature_ph[:, <span class="number">265</span>], tf.int32) <span class="comment"># 资源位</span></span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Embedding layer</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Embedding_layer'</span>):</span><br><span class="line">            self.uid_embeddings_var = tf.get_variable(<span class="string">"uid_embedding_var"</span>, [user_size, main_embedding_size])</span><br><span class="line">            tf.summary.histogram(<span class="string">'uid_embeddings_var'</span>, self.uid_embeddings_var)</span><br><span class="line">            <span class="comment"># 用户 embedding</span></span><br><span class="line">            self.uid_batch_embedded = tf.nn.embedding_lookup(self.uid_embeddings_var, self.uid) </span><br><span class="line"></span><br><span class="line">            self.mid_embeddings_var = tf.get_variable(<span class="string">"mid_embedding_var"</span>, [adgroup_id_size, main_embedding_size])</span><br><span class="line">            tf.summary.histogram(<span class="string">'mid_embeddings_var'</span>, self.mid_embeddings_var)</span><br><span class="line">            <span class="comment"># 广告单元 embedding</span></span><br><span class="line">            self. mid_batch_embedded = tf.nn.embedding_lookup(self.mid_embeddings_var, self.mid)</span><br><span class="line"></span><br><span class="line">            self.cat_embeddings_var = tf.get_variable(<span class="string">"cat_embedding_var"</span>, [cate_size, main_embedding_size])</span><br><span class="line">            tf.summary.histogram(<span class="string">'cat_embeddings_var'</span>, self.cat_embeddings_var)</span><br><span class="line">            <span class="comment"># 目标商品 embedding</span></span><br><span class="line">            self.cat_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cate_id)</span><br><span class="line">            <span class="comment"># 历史商品 embedding，与目标商品共享权重</span></span><br><span class="line">            self.cat_his_batch_embedded = tf.nn.embedding_lookup(self.cat_embeddings_var, self.cate_his)</span><br><span class="line"></span><br><span class="line">            self.brand_embeddings_var = tf.get_variable(<span class="string">"brand_embedding_var"</span>, [brand_size, main_embedding_size])</span><br><span class="line">            <span class="comment"># 目标商品的品牌 embedding</span></span><br><span class="line">            self.brand_batch_embedded = tf.nn.embedding_lookup(self.brand_embeddings_var, self.brand)</span><br><span class="line">            <span class="comment"># 历史商品的品牌 embedding，与目标商品共享权重</span></span><br><span class="line">            self.brand_his_batch_embedded = tf.nn.embedding_lookup(self.brand_embeddings_var, self.brand_his)</span><br><span class="line"></span><br><span class="line">            self.btag_embeddings_var = tf.get_variable(<span class="string">"btag_embedding_var"</span>, [btag_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 历史商品的行为 embedding，用于 I2I</span></span><br><span class="line">            self.btag_his_batch_embedded = tf.nn.embedding_lookup(self.btag_embeddings_var, self.btag_his)</span><br><span class="line">            self.dm_btag_embeddings_var = tf.get_variable(<span class="string">"dm_btag_embedding_var"</span>, [btag_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 历史商品的行为 embedding，用于 U2I，没有与 I2I 使用相同权重</span></span><br><span class="line">            self.dm_btag_his_batch_embedded = tf.nn.embedding_lookup(self.dm_btag_embeddings_var, self.btag_his)</span><br><span class="line"></span><br><span class="line">            self.campaign_id_embeddings_var = tf.get_variable(<span class="string">"campaign_id_embedding_var"</span>, [campaign_id_size, main_embedding_size])</span><br><span class="line">            <span class="comment"># 广告计划 embedding</span></span><br><span class="line">            self.campaign_id_batch_embedded = tf.nn.embedding_lookup(self.campaign_id_embeddings_var, self.campaign_id)</span><br><span class="line"></span><br><span class="line">            self.customer_embeddings_var = tf.get_variable(<span class="string">"customer_embedding_var"</span>, [customer_size, main_embedding_size])</span><br><span class="line">            <span class="comment"># 广告主 embedding</span></span><br><span class="line">            self.customer_batch_embedded = tf.nn.embedding_lookup(self.customer_embeddings_var, self.customer)</span><br><span class="line"></span><br><span class="line">            self.cms_segid_embeddings_var = tf.get_variable(<span class="string">"cms_segid_embedding_var"</span>, [cms_segid_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 微群 embedding</span></span><br><span class="line">            self.cms_segid_batch_embedded = tf.nn.embedding_lookup(self.cms_segid_embeddings_var, self.cms_segid)</span><br><span class="line"></span><br><span class="line">            self.cms_group_id_embeddings_var = tf.get_variable(<span class="string">"cms_group_id_embedding_var"</span>, [cms_group_id_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># cms_group_id embedding</span></span><br><span class="line">            self.cms_group_id_batch_embedded = tf.nn.embedding_lookup(self.cms_group_id_embeddings_var, self.cms_group_id)</span><br><span class="line"></span><br><span class="line">            self.final_gender_code_embeddings_var = tf.get_variable(<span class="string">"final_gender_code_embedding_var"</span>, [final_gender_code_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 性别 embedding</span></span><br><span class="line">            self.final_gender_code_batch_embedded = tf.nn.embedding_lookup(self.final_gender_code_embeddings_var, self.final_gender_code)</span><br><span class="line"></span><br><span class="line">            self.age_level_embeddings_var = tf.get_variable(<span class="string">"age_level_embedding_var"</span>, [age_level_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 年龄 embedding</span></span><br><span class="line">            self.age_level_batch_embedded = tf.nn.embedding_lookup(self.age_level_embeddings_var, self.age_level)</span><br><span class="line"></span><br><span class="line">            self.pvalue_level_embeddings_var = tf.get_variable(<span class="string">"pvalue_level_embedding_var"</span>, [pvalue_level_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 消费档次 embedding</span></span><br><span class="line">            self.pvalue_level_batch_embedded = tf.nn.embedding_lookup(self.pvalue_level_embeddings_var, self.pvalue_level)</span><br><span class="line"></span><br><span class="line">            self.shopping_level_embeddings_var = tf.get_variable(<span class="string">"shopping_level_embedding_var"</span>, [shopping_level_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 消费深度 embedding</span></span><br><span class="line">            self.shopping_level_batch_embedded = tf.nn.embedding_lookup(self.shopping_level_embeddings_var, self.shopping_level)</span><br><span class="line"></span><br><span class="line">            self.occupation_embeddings_var = tf.get_variable(<span class="string">"occupation_embedding_var"</span>, [occupation_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 是否大学生 embedding</span></span><br><span class="line">            self.occupation_batch_embedded = tf.nn.embedding_lookup(self.occupation_embeddings_var, self.occupation)</span><br><span class="line"></span><br><span class="line">            self.new_user_class_level_embeddings_var = tf.get_variable(<span class="string">"new_user_class_level_embedding_var"</span>, [new_user_class_level_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 城市层级 embedding</span></span><br><span class="line">            self.new_user_class_level_batch_embedded = tf.nn.embedding_lookup(self.new_user_class_level_embeddings_var, self.new_user_class_level)</span><br><span class="line"></span><br><span class="line">            self.pid_embeddings_var = tf.get_variable(<span class="string">"pid_embedding_var"</span>, [pid_size, other_embedding_size])</span><br><span class="line">            <span class="comment"># 资源位 embedding</span></span><br><span class="line">            self.pid_batch_embedded = tf.nn.embedding_lookup(self.pid_embeddings_var, self.pid)</span><br><span class="line">		   </span><br><span class="line">            <span class="comment"># 这里的 ‘+’ 是指拼接操作</span></span><br><span class="line">            <span class="comment"># 用户特征 = 用户id + 微群 + 性别 + 年龄 + 消费档次 + 消费深度 + 是否大学生 + 城市层级</span></span><br><span class="line">            self.user_feat = tf.concat([self.uid_batch_embedded, self.cms_segid_batch_embedded, self.cms_group_id_batch_embedded, self.final_gender_code_batch_embedded, self.age_level_batch_embedded, self.pvalue_level_batch_embedded, self.shopping_level_batch_embedded, self.occupation_batch_embedded, self.new_user_class_level_batch_embedded], <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 历史数据 embedding = 商品 + 品牌</span></span><br><span class="line">            self.item_his_eb = tf.concat([self.cat_his_batch_embedded, self.brand_his_batch_embedded], <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 历史数据总和 embedding = 所有历史数据相加</span></span><br><span class="line">            self.item_his_eb_sum = tf.reduce_sum(self.item_his_eb, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 目标商品特征 = 广告单元 + 商品 + 品牌 + 广告计划 + 广告主 + 价格</span></span><br><span class="line">            <span class="comment"># 这一个特征论文中没有提到，后面同其他特征一起送入全连接层用于分类。</span></span><br><span class="line">            self.item_feat = tf.concat([self.mid_batch_embedded, self.cat_batch_embedded, self.brand_batch_embedded, self.campaign_id_batch_embedded, self.customer_batch_embedded, self.price], <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 目标商品的 embedding = 商品 + 品牌</span></span><br><span class="line">            self.item_eb = tf.concat([self.cat_batch_embedded, self.brand_batch_embedded], <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 上下文特征 = 资源位</span></span><br><span class="line">            self.context_feat = self.pid_batch_embedded</span><br><span class="line"></span><br><span class="line">            self.lr = lr</span><br><span class="line">            self.global_step = global_step</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_fcn_net</span><span class="params">(self, inp)</span>:</span></span><br><span class="line">        <span class="comment"># 最后的全连接及分类层</span></span><br><span class="line">        inp = tf.layers.batch_normalization(inputs=inp, name=<span class="string">'bn_inp'</span>, training=<span class="literal">True</span>)</span><br><span class="line">        dnn0 = tf.layers.dense(inp, <span class="number">512</span>, activation=<span class="literal">None</span>, name=<span class="string">'f0'</span>)</span><br><span class="line">        dnn0 = prelu(dnn0, <span class="string">'prelu0'</span>)</span><br><span class="line">        dnn1 = tf.layers.dense(dnn0, <span class="number">256</span>, activation=<span class="literal">None</span>, name=<span class="string">'f1'</span>)</span><br><span class="line">        dnn1 = prelu(dnn1, <span class="string">'prelu1'</span>)</span><br><span class="line">        dnn2 = tf.layers.dense(dnn1, <span class="number">128</span>, activation=<span class="literal">None</span>, name=<span class="string">'f2'</span>)</span><br><span class="line">        dnn2 = prelu(dnn2, <span class="string">'prelu2'</span>)</span><br><span class="line">        dnn3 = tf.layers.dense(dnn2, <span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">'f3'</span>)</span><br><span class="line">        self.y_hat = tf.nn.sigmoid(dnn3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Metrics'</span>):</span><br><span class="line">            <span class="keyword">if</span> self.target_ph <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># Cross-entropy loss and optimizer initialization</span></span><br><span class="line">                ctr_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.target_ph, logits=tf.reduce_sum(dnn3, <span class="number">1</span>)))</span><br><span class="line">                self.ctr_loss = ctr_loss</span><br><span class="line">                self.loss = ctr_loss + self.aux_loss</span><br><span class="line">                tf.summary.scalar(<span class="string">'loss'</span>, self.loss)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Accuracy metric</span></span><br><span class="line">                self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(self.y_hat), self.target_ph), tf.float32))</span><br><span class="line">                tf.summary.scalar(<span class="string">'accuracy'</span>, self.accuracy)</span><br><span class="line"></span><br><span class="line">            self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)</span><br><span class="line">            <span class="keyword">with</span> tf.control_dependencies(self.update_ops):</span><br><span class="line">                self.training_op = self.optimizer.minimize(self.loss, global_step=self.global_step)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, sess, features, targets)</span>:</span></span><br><span class="line">        <span class="comment"># 定义训练阶段</span></span><br><span class="line">        loss, accuracy, aux_loss, _, probs = sess.run([self.loss, self.accuracy, self.aux_loss, self.training_op, self.y_hat], feed_dict=&#123;</span><br><span class="line">            self.feature_ph: features,</span><br><span class="line">            self.target_ph: targets</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">return</span> loss, accuracy, aux_loss, probs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self, sess, features, targets)</span>:</span></span><br><span class="line">        <span class="comment"># 定义测试阶段</span></span><br><span class="line">        loss, accuracy, aux_loss, probs = sess.run([self.loss, self.accuracy, self.aux_loss, self.y_hat], feed_dict=&#123;</span><br><span class="line">            self.feature_ph: features,</span><br><span class="line">            self.target_ph: targets</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="keyword">return</span> loss, accuracy, aux_loss, probs</span><br></pre></td></tr></table></figure>
<h2 id="Model-DMR"><a href="#Model-DMR" class="headerlink" title="Model_DMR"></a>Model_DMR</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model_DMR</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr, global_step)</span>:</span></span><br><span class="line">        super(Model_DMR, self).__init__(lr, global_step)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用 50 个历史数据</span></span><br><span class="line">        self.position_his = tf.range(<span class="number">50</span>)</span><br><span class="line">        self.position_embeddings_var = tf.get_variable(<span class="string">"position_embeddings_var"</span>, [<span class="number">50</span>, other_embedding_size])</span><br><span class="line">        <span class="comment"># I2I的 位置 embedding，shape = 50，8</span></span><br><span class="line">        self.position_his_eb = tf.nn.embedding_lookup(self.position_embeddings_var, self.position_his)  <span class="comment"># T,E</span></span><br><span class="line">        <span class="comment"># 复制了 batch 个，shape = batch * 50, 8</span></span><br><span class="line">        self.position_his_eb = tf.tile(self.position_his_eb, [tf.shape(self.mid)[<span class="number">0</span>], <span class="number">1</span>])  <span class="comment"># B*T,E</span></span><br><span class="line">        <span class="comment"># 变换维度，shape = batch，50，8，这样保证每个样本都有一个对应位置编码</span></span><br><span class="line">        self.position_his_eb = tf.reshape(self.position_his_eb, [tf.shape(self.mid)[<span class="number">0</span>], <span class="number">-1</span>, self.position_his_eb.get_shape().as_list()[<span class="number">1</span>]])  <span class="comment"># B,T,E</span></span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 用于 U2I 的位置 embedding，shape = 50，8，与用于 I2I 的位置 embedding 是不同的</span></span><br><span class="line">        self.dm_position_his = tf.range(<span class="number">50</span>)</span><br><span class="line">        self.dm_position_embeddings_var = tf.get_variable(<span class="string">"dm_position_embeddings_var"</span>, [<span class="number">50</span>, other_embedding_size])</span><br><span class="line">        <span class="comment"># 操作同 I2I </span></span><br><span class="line">        self.dm_position_his_eb = tf.nn.embedding_lookup(self.dm_position_embeddings_var, self.dm_position_his)  <span class="comment"># T,E</span></span><br><span class="line">        <span class="comment"># 操作同 I2I </span></span><br><span class="line">        self.dm_position_his_eb = tf.tile(self.dm_position_his_eb, [tf.shape(self.mid)[<span class="number">0</span>], <span class="number">1</span>])  <span class="comment"># B*T,E</span></span><br><span class="line">        <span class="comment"># 操作同 I2I </span></span><br><span class="line">        self.dm_position_his_eb = tf.reshape(self.dm_position_his_eb, [tf.shape(self.mid)[<span class="number">0</span>], <span class="number">-1</span>, self.dm_position_his_eb.get_shape().as_list()[<span class="number">1</span>]])  <span class="comment"># B,T,E</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># I2I 的位置 embedding = I2I 位置 embedding + I2I 历史商品的行为</span></span><br><span class="line">        <span class="comment"># 与其说是位置的 embedding，更不如说是 query 的构建，因为其不仅包含位置，还包含行为</span></span><br><span class="line">        self.position_his_eb = tf.concat([self.position_his_eb, self.btag_his_batch_embedded], <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># U2I 的位置 embedding = U2I 位置 embedding + U2I 历史商品的行为</span></span><br><span class="line">        self.dm_position_his_eb = tf.concat([self.dm_position_his_eb, self.dm_btag_his_batch_embedded], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># User-to-Item Network</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'u2i_net'</span>):</span><br><span class="line">            <span class="comment"># U2I 的 embedding 矩阵，注意这里并不是目标的 embedding 的结果</span></span><br><span class="line">            dm_item_vectors = tf.get_variable(<span class="string">"dm_item_vectors"</span>, [cate_size, main_embedding_size])</span><br><span class="line">            tf.summary.histogram(<span class="string">'dm_item_vectors'</span>, dm_item_vectors)</span><br><span class="line">            <span class="comment"># U2I 的 embedding 矩阵的偏置</span></span><br><span class="line">            <span class="comment"># TODO 这里不懂为什么设置一个不能训练的偏执</span></span><br><span class="line">            dm_item_biases = tf.get_variable(<span class="string">'dm_item_biases'</span>, [cate_size], initializer=tf.zeros_initializer(), trainable=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># Auxiliary Match Network</span></span><br><span class="line">            <span class="comment"># aux_loss 用于优化网络</span></span><br><span class="line">            <span class="comment"># dm_user_vector 用于计算相关性</span></span><br><span class="line">            <span class="comment"># scores 是每一个历史行为的 attention 系数</span></span><br><span class="line">            self.aux_loss, dm_user_vector, scores = deep_match(self.item_his_eb, self.dm_position_his_eb, self.mask, tf.cast(self.match_mask, tf.float32), self.cate_his, main_embedding_size, dm_item_vectors, dm_item_biases, cate_size)</span><br><span class="line">            <span class="comment"># beta = 0.1</span></span><br><span class="line">            self.aux_loss *= <span class="number">0.1</span></span><br><span class="line">            <span class="comment"># 得到经过辅助网络训练后的目标 embedding</span></span><br><span class="line">            dm_item_vec = tf.nn.embedding_lookup(dm_item_vectors, self.cate_id)  <span class="comment"># B,E</span></span><br><span class="line">            <span class="comment"># 用户向量与目标向量内积建模相关性</span></span><br><span class="line">            rel_u2i = tf.reduce_sum(dm_user_vector * dm_item_vec, axis=<span class="number">-1</span>, keep_dims=<span class="literal">True</span>)  <span class="comment"># B,1</span></span><br><span class="line">            self.rel_u2i = rel_u2i</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Item-to-Item Network</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'i2i_net'</span>):</span><br><span class="line">            <span class="comment"># att_outputs 是代表了用户兴趣的 embedding，alphas 是经过 norm 的 attention 系数</span></span><br><span class="line">            <span class="comment"># scores_unnorm 是没有经过 norm 的 attention 系数</span></span><br><span class="line">            att_outputs, alphas, scores_unnorm = dmr_fcn_attention(self.item_eb, self.item_his_eb, self.position_his_eb, self.mask)</span><br><span class="line">            tf.summary.histogram(<span class="string">'att_outputs'</span>, alphas)</span><br><span class="line">            <span class="comment"># I2I 得到的相关系数</span></span><br><span class="line">            rel_i2i = tf.expand_dims(tf.reduce_sum(scores_unnorm, [<span class="number">1</span>,<span class="number">2</span>]), <span class="number">-1</span>)</span><br><span class="line">            self.rel_i2i = rel_i2i</span><br><span class="line">            <span class="comment"># TODO：很奇怪这里所有 norm 后的 attention 系数加起来不应该是等于 1 的吗，不明白这里的用处</span></span><br><span class="line">            self.scores = tf.reduce_sum(alphas, <span class="number">1</span>)</span><br><span class="line">		</span><br><span class="line">        <span class="comment"># 最后总的特征 = 用户特征 + 目标特征 + 上下文特征 + 历史数据总和 +（目标商品 * 历史数据总和）</span></span><br><span class="line">        <span class="comment"># + U2I 相似性 + I2I 相似性 + 用户兴趣 </span></span><br><span class="line">        <span class="comment"># 用户特征、目标特征、上下文特征、历史数据总和、目标数据*历史数据总和，以上这些都是通过 embedding  		# 直接计算得到。U2I 相似性、I2I 相似性和用户兴趣都是通过子网络得到的输出</span></span><br><span class="line">        inp = tf.concat([self.user_feat, self.item_feat, self.context_feat, self.item_his_eb_sum,self.item_eb * self.item_his_eb_sum, rel_u2i, rel_i2i, att_outputs], <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 调用最后分类层</span></span><br><span class="line">        self.build_fcn_net(inp)</span><br></pre></td></tr></table></figure>
<h1 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h1><h2 id="prelu"><a href="#prelu" class="headerlink" title="prelu"></a>prelu</h2><p>实现了 prelu 函数，在模型中用作激活函数。</p>
<h2 id="deep-match"><a href="#deep-match" class="headerlink" title="deep_match"></a>deep_match</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_match</span><span class="params">(item_his_eb, context_his_eb, mask, match_mask, mid_his_batch, EMBEDDING_DIM, item_vectors, item_biases, n_mid)</span>:</span></span><br><span class="line">    query = context_his_eb</span><br><span class="line">    <span class="comment"># 将 query 的维度与 item 进行统一</span></span><br><span class="line">    query = tf.layers.dense(query, item_his_eb.get_shape().as_list()[<span class="number">-1</span>], activation=<span class="literal">None</span>, name=<span class="string">'dm_align'</span>)</span><br><span class="line">    <span class="comment"># 非线性变换</span></span><br><span class="line">    query = prelu(query, scope=<span class="string">'dm_prelu'</span>)</span><br><span class="line">    <span class="comment"># 使用 MLP 建模 attention</span></span><br><span class="line">    inputs = tf.concat([query, item_his_eb, query-item_his_eb, query*item_his_eb], axis=<span class="number">-1</span>) <span class="comment"># B,T,E</span></span><br><span class="line">    att_layer1 = tf.layers.dense(inputs, <span class="number">80</span>, activation=tf.nn.sigmoid, name=<span class="string">'dm_att_1'</span>)</span><br><span class="line">    att_layer2 = tf.layers.dense(att_layer1, <span class="number">40</span>, activation=tf.nn.sigmoid, name=<span class="string">'dm_att_2'</span>)</span><br><span class="line">    att_layer3 = tf.layers.dense(att_layer2, <span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">'dm_att_3'</span>)  <span class="comment"># B,T,1</span></span><br><span class="line">    <span class="comment"># 每一个位置都有一个 attention 系数</span></span><br><span class="line">    scores = tf.transpose(att_layer3, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]) <span class="comment"># B,1,T</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># mask</span></span><br><span class="line">    bool_mask = tf.equal(mask, tf.ones_like(mask))  <span class="comment"># B,T</span></span><br><span class="line">    key_masks = tf.expand_dims(bool_mask, <span class="number">1</span>)  <span class="comment"># B,1,T</span></span><br><span class="line">    paddings = tf.ones_like(scores) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取 mask 为 1 的时间点的 attention 系数，其他位置设置一个大负值</span></span><br><span class="line">    scores = tf.where(key_masks, scores, paddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># tril</span></span><br><span class="line">    <span class="comment"># 这里实现的功能简单概括，可以看作是对不同时间段的历史行为做 sum pooling</span></span><br><span class="line">    <span class="comment"># 具体的实现方法是，先将 scores 复制 T 行形成 T*T 的 attention 矩阵，</span></span><br><span class="line">    <span class="comment"># 列代表每个行为的 attention 系数，每一行都相同</span></span><br><span class="line">    <span class="comment"># 保留矩阵的下三角，给上三角赋值一个大的负值，并对矩阵做 softmax 运算</span></span><br><span class="line">    <span class="comment"># 再将这个权重矩阵（B, T, T）与历史行为矩阵（B, T, E）做矩阵乘法</span></span><br><span class="line">    <span class="comment"># 这样就得到添加了 attention 的历史行为（B, T, E）</span></span><br><span class="line">    <span class="comment"># 设 attention 的值为</span></span><br><span class="line">    <span class="comment"># [1, 1, 1]</span></span><br><span class="line">    <span class="comment"># 这里与文中提到的一个 trick 对应，行为序列是逆序排放的</span></span><br><span class="line">    <span class="comment"># 也就是说第一行是最近发生的，最后一行是最早发生的</span></span><br><span class="line">    <span class="comment"># 设历史行为的 embedding 为 </span></span><br><span class="line">    <span class="comment"># [1, 1, 1]</span></span><br><span class="line">    <span class="comment"># [2, 2, 2] </span></span><br><span class="line">    <span class="comment"># [3, 3, 3] </span></span><br><span class="line">    <span class="comment"># 则 attention 矩阵为 </span></span><br><span class="line">    <span class="comment"># 1      0      0</span></span><br><span class="line">    <span class="comment"># 1      1      0</span></span><br><span class="line">    <span class="comment"># 1      1      1</span></span><br><span class="line">    <span class="comment"># 矩阵相乘的结果为</span></span><br><span class="line">    <span class="comment">#   1         1        1</span></span><br><span class="line">    <span class="comment">#  1+2       1+2      1+2</span></span><br><span class="line">    <span class="comment"># 1+2+3     1+2+3     1+2+3</span></span><br><span class="line">    <span class="comment"># 每一行都是一个经过 sum pooling 的 embedding，可以看到每个行为仅与它之后发生的行为做 sum pooling</span></span><br><span class="line">    <span class="comment"># 因此新的 embedding 是对不同时间段的 embedding 做 sum pooling 的结果</span></span><br><span class="line">    scores_tile = tf.tile(tf.reduce_sum(scores, axis=<span class="number">1</span>), [<span class="number">1</span>, tf.shape(scores)[<span class="number">-1</span>]]) <span class="comment"># B, T*T</span></span><br><span class="line">    scores_tile = tf.reshape(scores_tile, [<span class="number">-1</span>, tf.shape(scores)[<span class="number">-1</span>], tf.shape(scores)[<span class="number">-1</span>]]) <span class="comment"># B,T,T</span></span><br><span class="line">    diag_vals = tf.ones_like(scores_tile)  <span class="comment"># B, T, T</span></span><br><span class="line">    <span class="comment"># tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()</span></span><br><span class="line">    tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()</span><br><span class="line">    paddings = tf.ones_like(tril) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line">    scores_tile = tf.where(tf.equal(tril, <span class="number">0</span>), paddings, scores_tile)  <span class="comment"># B, T, T</span></span><br><span class="line">    scores_tile = tf.nn.softmax(scores_tile) <span class="comment"># B, T, T</span></span><br><span class="line">    att_dm_item_his_eb = tf.matmul(scores_tile, item_his_eb) <span class="comment"># B, T, E</span></span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 新的历史行为经过非线性变换</span></span><br><span class="line">    dnn_layer1 = tf.layers.dense(att_dm_item_his_eb, EMBEDDING_DIM, activation=<span class="literal">None</span>, name=<span class="string">'dm_fcn_1'</span>)</span><br><span class="line">    dnn_layer1 = prelu(dnn_layer1, <span class="string">'dm_fcn_1'</span>) <span class="comment"># B, T, E</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># target mask</span></span><br><span class="line">    <span class="comment"># 包含所有历史行为的 embedding</span></span><br><span class="line">    user_vector = dnn_layer1[:, <span class="number">-1</span>, :]</span><br><span class="line">    <span class="comment"># 使用最近的 n-1 个数据预测最早的历史数据</span></span><br><span class="line">    user_vector2 = dnn_layer1[:, <span class="number">-2</span>, :] * tf.reshape(match_mask, [<span class="number">-1</span>, tf.shape(match_mask)[<span class="number">1</span>], <span class="number">1</span>])[:, <span class="number">-2</span>, :]</span><br><span class="line">    num_sampled = <span class="number">2000</span></span><br><span class="line">    <span class="comment"># match 任务</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=item_vectors,</span><br><span class="line">                                                      biases=item_biases,</span><br><span class="line">                                                      labels=tf.cast(tf.reshape(mid_his_batch[:, <span class="number">-1</span>], [<span class="number">-1</span>, <span class="number">1</span>]), tf.int64),</span><br><span class="line">                                                      inputs=user_vector2,</span><br><span class="line">                                                      num_sampled=num_sampled,</span><br><span class="line">                                                      num_classes=n_mid,</span><br><span class="line">                                                      sampled_values=tf.nn.learned_unigram_candidate_sampler(tf.cast(tf.reshape(mid_his_batch[:, <span class="number">-1</span>], [<span class="number">-1</span>, <span class="number">1</span>]), tf.int64), <span class="number">1</span>, num_sampled, <span class="literal">True</span>, n_mid)</span><br><span class="line">                                                      ))</span><br><span class="line">    <span class="keyword">return</span> loss, user_vector, scores</span><br></pre></td></tr></table></figure>
<h2 id="dmr-fcn-attention"><a href="#dmr-fcn-attention" class="headerlink" title="dmr_fcn_attention"></a>dmr_fcn_attention</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dmr_fcn_attention</span><span class="params">(item_eb, item_his_eb, context_his_eb, mask, mode=<span class="string">'SUM'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 将 mask 转为布尔类型</span></span><br><span class="line">    mask = tf.equal(mask, tf.ones_like(mask))</span><br><span class="line">    <span class="comment"># 将目标 embedding 复制 T 个</span></span><br><span class="line">    item_eb_tile = tf.tile(item_eb, [<span class="number">1</span>, tf.shape(mask)[<span class="number">1</span>]]) <span class="comment"># B, T*E</span></span><br><span class="line">    <span class="comment"># 转成（B, T, E）维度</span></span><br><span class="line">    item_eb_tile = tf.reshape(item_eb_tile, [<span class="number">-1</span>, tf.shape(mask)[<span class="number">1</span>], item_eb.shape[<span class="number">-1</span>]]) <span class="comment"># B, T, E</span></span><br><span class="line">    <span class="comment"># 如果没传入 query，就把目标当作 query</span></span><br><span class="line">    <span class="keyword">if</span> context_his_eb <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        query = item_eb_tile</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        query = tf.concat([item_eb_tile, context_his_eb], axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 统一 query 和目标的维度及进行非线性变换</span></span><br><span class="line">    query = tf.layers.dense(query, item_his_eb.get_shape().as_list()[<span class="number">-1</span>], activation=<span class="literal">None</span>, name=<span class="string">'dmr_align'</span>)</span><br><span class="line">    query = prelu(query, scope=<span class="string">'dmr_prelu'</span>)</span><br><span class="line">    <span class="comment"># 使用 MLP 建模 attention</span></span><br><span class="line">    dmr_all = tf.concat([query, item_his_eb, query-item_his_eb, query*item_his_eb], axis=<span class="number">-1</span>)</span><br><span class="line">    att_layer_1 = tf.layers.dense(dmr_all, <span class="number">80</span>, activation=tf.nn.sigmoid, name=<span class="string">'tg_att_1'</span>)</span><br><span class="line">    att_layer_2 = tf.layers.dense(att_layer_1, <span class="number">40</span>, activation=tf.nn.sigmoid, name=<span class="string">'tg_att_2'</span>)</span><br><span class="line">    att_layer_3 = tf.layers.dense(att_layer_2, <span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">'tg_att_3'</span>) <span class="comment"># B, T, 1</span></span><br><span class="line">    att_layer_3 = tf.reshape(att_layer_3, [<span class="number">-1</span>, <span class="number">1</span>, tf.shape(item_his_eb)[<span class="number">1</span>]]) <span class="comment"># B,1,T</span></span><br><span class="line">    scores = att_layer_3</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask</span></span><br><span class="line">    <span class="comment"># 这里与 U2I 中的 mask 类似，唯一不同是多了一个未经 softmax 的 socres 用于计算 alpha</span></span><br><span class="line">    key_masks = tf.expand_dims(mask, <span class="number">1</span>)  <span class="comment"># B,1,T</span></span><br><span class="line">    paddings = tf.ones_like(scores) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line">    paddings_no_softmax = tf.zeros_like(scores)</span><br><span class="line">    scores = tf.where(key_masks, scores, paddings)  <span class="comment"># [B, 1, T]</span></span><br><span class="line">    scores_no_softmax = tf.where(key_masks, scores, paddings_no_softmax)</span><br><span class="line"></span><br><span class="line">    scores = tf.nn.softmax(scores)</span><br><span class="line">    <span class="comment"># 如果是 SUM 模式，output 为所有历史行为的加权和</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'SUM'</span>:</span><br><span class="line">        output = tf.matmul(scores, item_his_eb)  <span class="comment"># [B, 1, H]</span></span><br><span class="line">        output = tf.reduce_sum(output, axis=<span class="number">1</span>)  <span class="comment"># B,E</span></span><br><span class="line">    <span class="comment"># 否则返回所有加权过的历史行为</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        scores = tf.reshape(scores, [<span class="number">-1</span>, tf.shape(item_his_eb)[<span class="number">1</span>]])</span><br><span class="line">        output = item_his_eb * tf.expand_dims(scores, <span class="number">-1</span>)</span><br><span class="line">        output = tf.reshape(output, tf.shape(item_his_eb))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, scores, scores_no_softmax</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DMR/" rel="tag"># DMR</a>
              <a href="/tags/CTR/" rel="tag"># CTR</a>
              <a href="/tags/code/" rel="tag"># code</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2020/05/07/DMR-art/" rel="next" title="DMR：深度匹配排名模型用于个性化点击率预测">
      DMR：深度匹配排名模型用于个性化点击率预测 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#data-iterator-py"><span class="nav-number">1.</span> <span class="nav-text">data_iterator.py</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#model-py"><span class="nav-number">2.</span> <span class="nav-text">model.py</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model"><span class="nav-number">2.1.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-DMR"><span class="nav-number">2.2.</span> <span class="nav-text">Model_DMR</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#utils-py"><span class="nav-number">3.</span> <span class="nav-text">utils.py</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#prelu"><span class="nav-number">3.1.</span> <span class="nav-text">prelu</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#deep-match"><span class="nav-number">3.2.</span> <span class="nav-text">deep_match</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dmr-fcn-attention"><span class="nav-number">3.3.</span> <span class="nav-text">dmr_fcn_attention</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">谭廷一</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">谭廷一</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
